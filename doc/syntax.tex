
\section{Introduction}

Nowadays the software programmer writing an embedded DSL~\cite{hudak1996building}
and the PL researcher formalising a calculus both know and
leverage the host language's type system. Using Generalised
Algebraic Data Types (GADTs) or the more general indexed
families of Type Theory~\cite{dybjer1994inductive} for their deep embedding, they can
\emph{statically} enforce some of the invariants present in
their language. Managing the scope is a popular use case~\cite{altenkirch1999monadic} as
directly manipulating raw de Bruijn indices is error-prone;
solutions range from enforced well scopedness to full type
and scope correctness.

This paper starts with primers on scope safe terms, a scope and type
safe variant as well as invariant preserving programs acting on them
and a generic way to represent data types. These introductory sections
help us build an understanding of the problem at hand as well as a
toolkit that leads us to the original content of this paper: a universe
of scope safe syntaxes with binding together with a generic notion of
scope safe semantics for these syntaxes.
This gives us the opportunity to write generic implementations of renaming,
substitution but also elaboration of a surface language to a core one,
and normalisation by evaluation. We also explore opportunities for
generic proving by describing a framework to formally describe what
it means for a semantics to be able to simulate another one.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SCOPE SAFE TERMS


\section{A Primer on Scope Safe Terms}\label{section:primer-term}

Scope safe terms are following a strict discipline which enforces statically
that they may only refer to variables introduced by a binder beforehand. A
scope safe language is a programming language in which all the valid terms
are guaranteed to be scope safe.

Bellegarde and Hook~\citeyear{BELLEGARDE1994287}, Bird and Patterson~\citeyear{bird_paterson_1999},
and Altenkirch and Reus~\citeyear{altenkirch1999monadic} introduced the
nowadays classic presentation of scope safe languages using inductive
\emph{families}~\cite{dybjer1994inductive} to track scoping information
at the type level. Instead of describing the type of abstract syntax trees
of the language as the fixpoint of an endofunctor on $\Set{}$, they used
an endofunctor on $\Set{}^{\Set{}}$ where the $\Set{}$ index corresponds
to the set of variables in scope. Because the empty Set has no inhabitant,
it is a natural representation of the empty scope. Conversely, the functor
$M(X) = 1 + X$ is used to extend the running scope with an extra variable.

This generic presentation of scope safe languages leads to the following
definition of the untyped $\lambda$-calculus. The endofunctor
$T(F) = \lambda X \in \Set{}. X + (F(X) \times F(X)) + F(1 + X)$
offers a choice of three constructors. The first one corresponds to the variable
case; it packages an inhabitant of $X$, the index $\Set{}$. The second corresponds
to an application node; both the function and its argument live in the same
scope as the overall expression. Last but not least, the third corresponds to
a $\lambda$-abstraction; it extends the current scope with a fresh variable.
The language is obtained as the fixpoint of $T$:
\[
   \mathit{Lam} = \mu F \in \Set{}^{\Set{}}.
   \lambda X \in \Set{}. X + (F(X) \times F(X)) + F(1 + X)
\]
The proof that the fixpoint is functorial then corresponds to renaming
whilst the proof that it is monadic implements substitution: the variable
constructor is return, and bind defines parallel substitution.

\subsection{A Mechanized Typed Variant of Altenkirch and Reus' Calculus}

There is no reason to restrict this technique to fixpoints of endofunctors
on $\Set{}^{\Set{}}$ apart from the fact that renaming and substitution
correspond to well-known structures in that specific case. The more general
case of fixpoints of (strictly positive) endofunctors on $\Set{}^J$ can be
endowed with similar operations by using what Altenkirch, Chapman and
Uustalu~\citeyear{Altenkirch2010, JFR4389} refer to as relative monads.

In this paper, we pick $J = \mathit{List} I$ which is straightforwardly the list
of the sort (/ kind / types depending on the application) of the de Bruijn variables
in scope. We can recover an untyped approach by picking $I$ to be the unit type.
Because of this typed setting, our functors will actually take an extra $I$ argument
corresponding to the type of the expression being built. This is summed up by
the large type \AB{I}\AF{‚îÄScoped}:

\begin{figure}[h]
\ExecuteMetaData[var.tex]{scoped}
\caption{Type of Well \AB{I}-Kinded and Well Scoped Families}
\end{figure}

Our implementation language is Agda~\cite{norell2009dependently} however
these techniques are language independent: any dependently typed language
whose logic is at least as powerful as Martin-L\"of Type
Theory~\cite{martin1982constructive} equipped with inductive
families~\cite{dybjer1994inductive} ought to do.

In order to lighten the presentation, we weaponise the observation that the
current scope is either threaded to subterms (e.g. in the application's case)
or adjusted (e.g. in the $\lambda$-abstraction's case) by introducing combinators
to build indexed types. Although it may seem surprising at first to define
infix operators of arity three, they are meant to be used partially applied,
surrounded by \AF{[\_]} which turns an indexed Set into a Set by implicitly
quantifying over the index. The first two combinators are the pointwise liftings
of the function space and the product type respectively, both silently threading
the underlying scope. The third one is simply the constant function turning a Set
into an indexed Set by ignoring the index. The fourth one makes explicit
the \emph{adjustment} made to the index by a function. We use Agda's mixfix
operator notation (where underscores denote argument positions) to suggest its
connection to the mathematical convention of only mentioning context \emph{extensions}
when presenting judgements (see e.g. \cite{martin1982constructive}) and write
\AB{f} \AF{‚ä¢} \AB{T} where \AB{f} is the modification and \AB{T} the indexed set
it operates on. The last one implicitly quantifies over the index,
turning a family into a Set.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[indexed.tex]{arrow}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[indexed.tex]{product}
\end{minipage}

\begin{minipage}{0.25\textwidth}
\ExecuteMetaData[indexed.tex]{constant}
\end{minipage}
\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[indexed.tex]{adjust}
\end{minipage}\hspace{2em}
\begin{minipage}{0.25\textwidth}
\ExecuteMetaData[indexed.tex]{forall}
\end{minipage}
\caption{Combinators to build indexed Sets}\label{figure:indexed}
\end{figure}


These combinators lead to more readable type declarations. For instance,
the compact expression
\AF{[} \AF{suc} \AF{‚ä¢} (\AB{P} \AF{‚àô√ó} \AB{Q}) \AF{‚ü∂} \AB{R} \AF{]}
desugars to the more verbose type
\AS{‚àÄ} \{\AB{i}\} \AS{‚Üí} (\AB{P} (\AF{suc} \AB{i}) \AF{√ó} \AB{Q} (\AF{suc} \AB{i})) \AS{‚Üí} \AB{R} \AB{i}.

Because the context comes second in the definition of \AF{\_‚îÄScoped}, we
can readily use these combinators to thread, modify or quantify over the
scope when defining such families:

\begin{figure}[h]
\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[var.tex]{var}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\ExecuteMetaData[motivation.tex]{tm}
\end{minipage}
\caption{Scope Aware Variables and Simply Typed $\lambda$-Terms\label{scoped-untyped}}
\end{figure}

The inductive family \AD{Var} corresponds to well scoped and well kinded
de Bruijn~\citeyear{de1972lambda}
indices. Its first constructor (\AIC{z} for zero) states that we have a name to refer to
the nearest binder in a non-empty scope. The second one (\AIC{s} for successor) lifts a
name for a variable in a given scope into a name for it in the extended scope where
an extra variable has been bound. Both of their types have been written using combinators.
Although we will abstain from unfolding them in the future, we do so here in the hope
it will help the reader get acquainted with them: they respectively normalise to
$\forall i, xs. \mathbf{Var}(i, i :\!: \mathit{xs})$ for \AIC{z},
and $\forall i, j, xs. \mathbf{Var}(i, \mathit{xs}) \rightarrow \mathbf{Var}(i, j :\!: \mathit{xs})$ for \AIC{s}.

The \AD{Type} \AF{‚îÄScoped} family \AD{Lam} is the simply typed variant of Altenkirch
and Reus' untyped $\lambda$-calculus. The two interesting constructors are the one
lifting variables to terms and the $\lambda$-abstraction whose body lives in an
extended context.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SCOPE SAFE PROGRAMS


\section{A Primer on Kind and Scope Safe Programs}\label{section:primer-program}

This type and scope safe deep embedding of the simply typed $\lambda$-calculus is
naturally only a start: once the programmer has access to a good
representation of the language she is interested in, she wants and
needs to (re)implement standard traversals manipulating terms.
Renaming and substitution are perhaps the two most iconic examples
of such traversals. And now that well typedness and scopedness is enforced in
the terms' indices, all of these traversals have to be implemented
in a type and scope safe manner. These constraints show up in the type of
renaming and substitution which can be defined as follows:

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[motivation.tex]{ren}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[motivation.tex]{sub}
\end{minipage}
\caption{Type and Scope Preserving Renaming and Substitution}
\end{figure}

We wrote (\AB{Œì} \AR{‚îÄEnv}) \AB{ùì•} \AB{Œî} for an environment with values
in \AB{ùì•}. It associates to each variable in the scope \AB{Œì} a value of
the appropriate type living in the scope \AB{Œî}. We define environemnts
as records to help the host language's type inference reconstruct the type
of values they store.

\begin{figure}[h]
\ExecuteMetaData[environment.tex]{env}
\caption{Well Typed and Scoped Environments of Values}
\end{figure}

Looking more closely at these two functions' code, it is quite evident
that they have a very similar structure. In each case, we have used
two (function specific) auxiliary definitions named \AF{‚ü¶V‚üß} and \AF{extend}
respectively to highlight this fact. Abstracting away this shared structure
would allow for these definitions to be refactored, and their common
properties to be proved in one swift move.

Previous efforts in dependently typed
programming~\cite{benton2012strongly,allais2017type}
have precisely achieved this goal and refactored renaming and substitution,
but also normalisation by evaluation, printing with names or CPS conversion
as various instances of a more general traversal. Unpublished results also
demonstrate that typechecking in the style of Atkey~\citeyear{atkey2015algebraic}
fits in that framework. To make sense of this body of work, we
need to introduce three new notions: \AF{Thinning}s, the \AF{‚ñ°} functor and
\AF{Thinnable}s.

\begin{figure}[h]
\ExecuteMetaData[environment.tex]{thinning}
\caption{Thinnings: A Special Case of Environments}
\end{figure}

\AF{Thinning}s subsume more structured notions such as the Category of
Weakenings~\cite{altenkirch1995categorical} or Order Preserving
Embeddings~\cite{chapman2009type}. In particular, they do not prevent the
user from defining arbitrary permutations or from introducing contractions
although we will not use such instances. Our representation is a function
space which grants us monoid laws ``for free'' as per Jeffrey's
observation~\citeyear{jeffrey2011assoc}.

The \AF{‚ñ°} combinator turns any (\AD{List} \AB{I})-indexed Set into one that can absorb
thinnings. It is akin to Kripke-style quantification over all possible future
worlds and \AF{‚ñ°} (\AB{D} \AF{‚Üí} \AB{D}) indeed corresponds to the Kripke
function space used in normalisation by evaluation via a domain \AB{D}.
Because we can define an identity \AF{Thinning} and \AF{Thinning}s do
compose, \AF{‚ñ°} is a comonad.

The notion of \AF{Thinnable} is the property of being stable under thinnings;
in other words \AF{Thinnable}s are the coalgebras of \AF{‚ñ°}.
It is a crucial property for values to have if one wants to be able to push
them under binders. Unsurprisingly, from the comonadic structure we get that
the \AF{‚ñ°} combinator freely turns any (\AD{List} I)-indexed Set into a
\AF{Thinnable} one.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[environment.tex]{box}
\ExecuteMetaData[environment.tex]{comonad}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[environment.tex]{thinnable}
\ExecuteMetaData[environment.tex]{freeth}
\end{minipage}
\caption{The \AF{‚ñ°} comonad, Thinnable, and the cofree Thinnable.}
\end{figure}

Equipped with these new notions, we can define an abstract
concept of semantics for our scope safe language. Broadly
speaking, a semantics turns our deeply embedded abstract
syntax trees into the shallow embedding of the corresponding
parametrised higher order abstract syntax term. We get various
semantics by using different 'host languages' for this shallow
embedding. A semantics with values in $\mathcal{V}$ and
computations in $\mathcal{C}$ is meant to give rise to a
traversal which, provided a term and an environment of values
for each one of the variables in scope, delivers a computation.

A Semantics is characterised by a set of constraints on the notions of
values $\mathcal{V}$ and computations $\mathcal{C}$ at hand. First of all,
values should be thinnable so that \AF{sem} may push the environment
under binders. Second, the set of computations needs to be closed
under various combinators which are the semantical counterparts of
the language's constructors. Here the semantical counterpart of
application is not particularly interesting. However the interpretation
of the $\lambda$-abstraction is of interest: it is a variant on
the Kripke function space one can find in normalisation by evaluation.
In all possible thinnings of the scope at hand, it promises to deliver
a computation whenever it is provided with a value for its newly
bound variable. This is concisely expressed by the type
(\AF{‚ñ°} (\AB{\mathcal{V}} \AF{‚Üí} \AB{\mathcal{C}})).

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{rsem}
\caption{Semantics for \AD{Lam}}
\end{figure}

The traversal \AF{sem} realises the promise made earlier that any given
{\AR{Sem} \AB{\mathcal{V}} \AB{\mathcal{C}}} will induce a function which,
given a value in $\mathcal{V}$ for each variable in scope, turns a term
into a computation $\mathcal{C}$.

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{sem}
\caption{Fundamental Lemma of Semantics for \AD{Lam}}
\end{figure}

Coming back to renaming and substitution, we can see that they both fit
in the \AR{Sem} framework. We can notice that the definition of Substitution
depends on the Renaming one: to be able to push terms under binder, we
need to have already proven that they are thinnable.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[motivation.tex]{semren}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\ExecuteMetaData[motivation.tex]{semsub}
\end{minipage}
\caption{Renaming and Substitution as Instances of \AR{Sem}}
\end{figure}

We also include the definition of a basic printer relying on a
name supply to highlight the fact that computations can very well be
effectful. The \AF{Printing} semantics is defined by using \AD{String}s
as values and {\AD{State} \AD{\mathbb{N}} \AD{String}} as computations. We use a
\AR{Wrap}per with a type and a context as phantom types in order to
help Agda's inference propagate the appropriate constraints.
One of the main driving force is the definition of \AF{fresh} which
generates a fresh name for a newly-bound variable.

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{valprint}
\ExecuteMetaData[motivation.tex]{freshprint}
\caption{Wrapper and fresh name generation}
\end{figure}

The wrapper \AR{Wrap} is trivially a thinnable functor so we omit here
the definitions of \AF{th^{Wrap}} and \AF{map^{Wrap}} and jump straight
to the definition of the printer. To print an application node, we produce
a string representation of the function, then of its argument and combine
them by putting the argument between parentheses. To print a $\lambda$-abstraction,
we start by generating a fresh name for the newly-bound variable, use that
name to generate a string representing the body of the function to which we
prepend a ``$\lambda$'' binding the fresh name.

\begin{figure}[h]
\ExecuteMetaData[motivation.tex]{semprint}
\caption{Printing as an instance of \AR{Sem}}
\end{figure}

Both Printing and Renaming highlight the importance of having a distinct
notion of values and computations: the type of values in their respective
environments are distinct from their type of computations.

All of these examples are already described at length by Allais, Chapman,
McBride and McKinna~\citeyear{allais2017type} so we will not spend any
more time on them. They have also obtained the simulation and fusion
theorems demonstrating that these traversals are well-behaved as
corollaries of more general results expressed in terms of \AF{sem}.
We will come back to this in Section~\ref{section:simulation}.

One important observation to make is the tight connection between the
constraint described by \AR{Sem} and the definition of \AD{Lam}: the
semantical combinators are related to the corresponding constructors
where the recursive occurences of the inductive family have been replaced
with either a computation or a Kripke function space whenever an
extra variable was bound. This suggests that it ought to be possible
to compute the definition of \AF{Sem} from the datatype's.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DATATYPES

\section{A Primer on the Universe of Data Types}

Chapman, Dagand, McBride and Morris~\citeyear{Chapman:2010:GAL:1863543.1863547}
defined a universe of data types inspired by Dybjer and Setzer's
finite axiomatisation of Inductive-Recursive definitions~\citeyear{Dybjer1999}
and Benke, Dybjer and Jansson's universes for generic programs and proofs~\citeyear{benke-ugpp}.
This explicit definition of \emph{codes} for data types empowers the
user to write generic programs tackling \emph{all} of the data types
one can obtain this way. In this section we recall the main aspects
of this construction we are interested in to build up our generic
representation of syntaxes with binding.

The first component of this universe's definition is an inductive type
of \AD{Desc}riptions of strictly positive functors from $\Set{}^J$ to
$\Set{}^I$. It has three constructors: one to store data (the rest of
the description can depend upon this stored value), one to attach a
recursive substructure indexed by $J$ and one to stop, enforcing the
value of the index the current structure uses.

The recursive function \AF{‚ü¶\_‚üß} makes the interpretation of the
Descriptions formal. They essentially give rise to equality
terminated right nested tuples of stored data and substructures.

\begin{figure}[h]
\hspace{-1em}\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[Generic/Data.tex]{desc}
\end{minipage}\hspace{2em}
\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[Generic/Data.tex]{interp}
\end{minipage}
\caption{Datatype Descriptions and their Meaning as Functors}\label{figure:desc}
\end{figure}

These constructors give the programmer the ability to build up the data
types she is used to, using a dummy index of unit type whenever she does
not need the extra expressivity. For instance, the functor corresponding
to lists of elements in $A$ stores one bit of data: whether the current
node is the empty list or not. Depending on that bit, the rest of the
description is either the ``stop'' token or a pair of an element in $A$
and a recursive substructure i.e. the tail of the list.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{listD}
\caption{The Description of the base functor for \AD{List} \AB{A}}\label{figure:listD}
\end{figure}

But she can also use the index to enforce invariants as in the definition
of the base functor for the \AD{Vec} \AB{A} \AB{n} inductive family of
length-indexed lists. It has the same structure as the definition of \AF{listD}.
We start with a bit of data distinguishing the two constructors: either
the empty list (in which case the branch's index is enforced to be $0$) or a
non-empty one in which case we store a natural number \AB{n}, the head of type
\AB{A} and a tail of size \AB{n} (and the branch's index is enforced to be
\AIC{suc} \AB{n}).

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{vecD}
\caption{The Description of the base functor for \AD{Vec} \AB{A} \AB{n}}\label{figure:vecD}
\end{figure}

After implementing \AF{‚ü¶\_‚üß}, which acts on the objects of $\Set{}^J$, we can
define the function \AF{fmap} by recursion over a code \AB{d}. It describes
the action of the functor corresponding to \AB{d} over morphisms in $\Set{}^J$.
This is the first example of generic programming over all the functors one can
obtain as the meaning of a description.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{fmap}
\caption{Action on Arrows of the Functor corresponding to a \AF{Descr}iption}
\end{figure}

All the functors obtained as meanings of \AD{Desc}riptions are strictly
positive. So we can build the least fixpoint of the ones that are endofunctors
(i.e. the ones for which $I$ equals $J$). This fixpoint is called \AD{Œº}
and its initiality is proven by the definition of \AF{fold} \AB{d}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Data.tex]{mu}
\ExecuteMetaData[Generic/Data.tex]{fold}
\caption{Least Fixpoint of an Endofunctor and Corresponding Generic Fold}
\end{figure}

Here the \AD{Size}~\cite{DBLP:journals/corr/abs-1012-4896} index added
to the inductive definition of \AD{Œº} plays a crucial role in getting
the termination checker to see that \AF{fold} is a total function.
Indeed the recursive calls to \AF{fold} are performed indirectly via
\AF{fmap} and it's only because the compiler knows that \AB{i}, the index
at which the recursive calls are performed, is strictly smaller than
\AF{‚Üë} \AB{i} that it accepts the definition as total. Without this type
based approach to termination checking the definition of \AF{fold} would
be rejected and our only recourse would be to manually inline \AF{fmap}.
However, in most definitions the \AD{Size} does not matter in which case
we will simply use the primitive limit \AD{Size} \AF{‚àû} which is
characterised by \AF{‚àû} = \AF{‚Üë} \AF{‚àû}.

This demonstrates that this approach allows us to define generically
the iteration principle associated to all the datatypes which arise
as the fixpoint of a description's meaning. It seems appropriate to
base our universe of scope safe syntaxes on a similar construction
so that we may be able to define generically a notion of semantics
for all the syntaxes with binding one may come up with. Chapman,
Dagand, McBride and Morris also give a more general universe which
supports higher order branching (but still does not have a notion
of variable). We decidedly stick to finitely branching constructors
(but still potentially infinitary ones because of the \AIC{`œÉ}
constructor) thus sticking to the common understanding of `syntax'.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% UNIVERSE OF SYNTAXES


\section{A Universe of Scope Safe and Well Kinded Syntaxes}

Our universe of scope safe and well kinded syntaxes follows the same principle
as Chapman, Dagand, McBride and Morris' universe of datatypes except that we
are not building endofunctors on $\Set{}$ anymore but rather
on \AB{I} \AF{‚îÄScoped}. Descriptions can be built using three constructors:
the first one makes it possible to store data (and, as usual, the rest of
the description may depend upon the value stored), the second takes a list
of the newly bound variables' kinds as well as the kind of the substructure
living in that extended scope and the last one ends the definition.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{desc}
\caption{Syntax Descriptions}
\end{figure}

The meaning function \AF{‚ü¶\_‚üß} we associate to a description is not
quite an endofunctor on \AB{I} \AF{‚îÄScoped}; it is more general than that.
Given an $X$ that interprets substructures of kind $j$ with an extra $Œî$ bound
variables in a scope that has $Œì$ bound variables already as $X\,Œî\,j\,Œì$,
we give a description a meaning as an \AB{I} \AF{‚îÄScoped}. The astute reader
may have noticed that \AF{‚ü¶\_‚üß} is uniform in $X$ and $Œì$; however
refactoring \AF{‚ü¶\_‚üß} to use the partially applied $X\,\_\,\_\,Œì$ following
this observation would lead to a definition harder to use with the
combinators for indexed sets described in Figure~\ref{figure:indexed}
which make our types much more readable.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{interp}
\caption{Decsription's Meanings as ``Functors''}
\end{figure}

If we pre-compose the meaning function \AF{‚ü¶\_‚üß} with a notion of `de Bruijn scopes'
(denoted \AF{Scope} here) which turns any \AB{I} \AF{‚îÄScoped} family into a function
of type \AD{List} \AB{I} \AS{‚Üí} \AB{I} \AF{‚îÄScoped} by simply appending the two
\AD{List} indices, we recover a meaning function producing an endofunctor on \AB{I} \AF{‚îÄScoped}.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{scope}
\caption{De Bruijn Scopes}
\end{figure}

The endofunctors thus defined are strictly positive and we can take their fixpoints.
Because we want to define the terms of a language with variables, instead of
considering the initial algebra, this time we opt for the free relative
monad~\cite{JFR4389} (with respect to the functor \AF{Var}): the \AIC{`var}
constructor corresponds to return, and we will define bind (also known as
the parallel substitution \AF{sub}) in the next section. We have once more
a \AD{Size} index to get all the benefits of type based termination checking.

\begin{figure}[h]
\ExecuteMetaData[Generic/Syntax.tex]{mu}
\caption{Term Trees: The Free \AF{Var}-Relative Monads on Descriptions}
\end{figure}

Coming back to our original examples, we now have the ability to give
codes for the well-scoped untyped $\lambda$-calculus and, just as well,
the intrinsically typed simply typed $\lambda$-calculus.
The variable case will be added by the free monad construction so we
only have to describe two constructors: application where we have two
substructures which do not bind any extra argument and $\lambda$-abstraction
which has exactly one substructure with precisely one extra bound variable.
In the untyped case a single bit is enough to distinguish the two constructors
whilst in the typed case, we need our tags to carry extra information about the
types involved so we use the ad-hoc \AD{`STLC} type.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/UntypedLC.tex]{LCD}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/STLC.tex]{stlc}
\end{minipage}
\caption{Examples: The Untyped and Simply Typed Lambda Calculi}
\end{figure}

We can then define constructors corresponding
to the original ones in Figure~\ref{scoped-untyped}: \AIC{`V}
for \AIC{V} the variable constructor, \AIC{`A} for \AIC{A} the
application one and \AIC{`L} for \AIC{L} the $\lambda$-abstraction.
In Agda, we can use pattern synonyms~\cite{Pickering:patsyn} to define these.
This means that the end user can seemlessly write pattern-matching programs on
encoded terms without dealing with the gnarly details of the encoding.
These pattern definitions can omit some arguments by using ``\AS{\_}'',
in which case they will be filled in by unification just like any other
implicit argument: there is no extra cost to using an encoding!
The only downside is that the language currently does not allow the
user to specify type annotations for her pattern synonyms.

\begin{figure}[h]
\begin{minipage}{0.40\textwidth}
 \ExecuteMetaData[Generic/Examples/UntypedLC.tex]{LCpat}
\end{minipage}\hspace{2em}
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[Generic/Examples/STLC.tex]{patST}
\end{minipage}
\caption{Respective Pattern Synonyms for the Untyped and Simply-Typed Lambda Calculus}
\end{figure}

It is the second time (the first time being the definition of
\AF{listD} in Figure~\ref{figure:listD}) that we use a
\AF{Bool} to distinguish between two constructors. In order
to avoid re-encoding the same logic,
the next section introduces combinators demonstrating that
descriptions are closed under finite sums and finite products
of recursive positions.

\subsection{Common Combinators and Their Properties}\label{desccomb}

As seen previously, we can take the coproduct \AF{\_`+\_} of two
descriptions by using a dependent pair whose first component
stores a \AF{Bool}ean tagging which branch was taken whilst
the second one uses that information to return the description
corresponding to that branch. We can define an appropriate
eliminator \AF{case} which given two continuations picks the
one corresponding to the chosen branch.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{sumcomb}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{case}
\end{minipage}
\caption{Descriptions are closed under Sums}
\end{figure}

Closure under product does not hold in general. Indeed, the
equality constraints introduced by the two end tokens of two
descriptions may be incompatible. So far, a limited form of
closure (closure under finite product of recursive positions)
has been sufficient for all of our use cases. As with coproducts,
the appropriate eliminator \AF{unXs} takes a value in the encoding
and extracts its constituents.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{paircomb}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Syntax.tex]{pairunpair}
\end{minipage}
\caption{Descriptions are closed under Finite Products of Recursive Positions}
\end{figure}

A concrete use case for both of these results will be given in section~\ref{section:letbinding}
where we explain how to seamlessly enrich an existing syntax with let-bindings
and how to use the \AR{Sem} framework to elaborate them away.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GENERIC SEMANTICS


\section{Generic Scope Safe and Well Kinded Programs for Syntaxes}\label{section:semantics}

Based on the structure made explicit in the example worked out
in Section~\ref{section:primer-program}, we can define a generic notion of
semantics for all syntax descriptions. It is once more parametrised
by two \AB{I}\AF{‚îÄScoped} families \AB{ùì•} and \AB{ùìí} corresponding
respectively to values associated to bound variables and
computations delivered by evaluating terms. These two families
have to abide by three constraints
\begin{itemize}
\item Values should be thinnable so that we can push the
      evaluation environment under binders;
\item Values should embed into Computations for us to be able
      to return the value associated to a variable as the
      result of its evaluation;
\item Last but not least, we should have an algebra turning
      a term where substructures have been replaced with
      either Computations or Kripke functional spaces (depending
      on whether extra bound variables have been introduced)
      into Computations
\end{itemize}

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{semantics}
\caption{A Generic Notion of Semantics}
\end{figure}

Here we crucially use the fact that the meaning of a description is
defined in terms of a function interpreting substructures which has
the type \AF{List} \AB{I} \AS{‚Üí} \AB{I}\AF{‚îÄScoped}, i.e. that gets access
to the current scope but also the exact list of the newly bound variables' kinds.
We define such a function, \AF{Kripke}, by case analysis on the number
of newly bound variables: if it's $0$ we expect the substructure
to be a computation (the result of the evaluation function's
recursive call) but if there are newly bound variables then we expect
that in any context extension, we will have a function which, provided
a value for each one of these extra variables, will deliver a computation
corresponding to the evaluation of the body of the binder in the extended
environment.

\begin{figure}[h]
\ExecuteMetaData[environment.tex]{kripke}
\caption{Substructures as either Computations or Kripke Function Spaces}
\end{figure}

It is once more the case that the abstract notion of Semantics comes
with a fundamental lemma: all \AB{I} \AF{‚îÄScoped} families \AB{ùì•} and
\AB{ùìí} satisfying the three criteria we have put forward give rise
to an evaluation function. We introduce a notion of computation
\AF{\_‚îÄComp} analogous to that of environments: instead of associating
values to variables, it associates computations to terms.

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{comp}
\caption{\AF{\_‚îÄComp}: Associating Computations to Terms}
\end{figure}

\subsection{Fundamental Lemma of Semantics}

We can then define the type of the fundamental lemma (called \AF{sem}) as
a function from environments to computations. It is defined mutually with a
function \AF{body} turning a de Bruijn \AF{Scope} (i.e. a substructure in a
potentially extended context) into a \AF{Kripke} (i.e. a subcomputation
expecting a value for each newly bound variable).

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{semtype}
\caption{Statement of the Fundamental Lemma of \AR{Sem}antics}
\end{figure}

The proof \AF{sem} of this fundamental lemma is straightforward now
that we have clearly identified what the problem's structure is and
which constraints to enforce. If the term considered is a variable,
we simply lookup the associated value in the evaluation environment
and turn it into a computation using \ARF{var}. If it is a proper
node then we call \AF{fmap} to evaluate the substructures using
\AF{body} and then call the \ARF{alg}ebra to combine these results.

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{sem}
\caption{Proof of the Fundamental Lemma of \AR{Sem}antics -- \AF{sem}}
\end{figure}

The auxiliary lemma \AF{body} distinguishes two cases: if no new
variable has been bound in the recursive substructure, it is simply
a matter of calling \AF{sem} recursively, otherwise we are provided
with a \AF{Thinning}, some additional values and evaluate the
substructure in the thinned and extended evaluation environment.

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{body}
\caption{Proof of the Fundamental Lemma of \AR{Sem}antics -- \AF{body}}
\end{figure}

Because \AF{fmap} introduces one level of indirection between the
recursive calls and the subterms they are acting upon, the fact
that our terms are indexed by a \AF{Size} is once more crucial in
getting the termination checker to see that our proof is indeed
trivially well-founded.

\subsection{Our First Generic Programs: Renaming and Substitution}

Renaming can be defined generically for all syntax descriptions as a
semantics with \AF{Var} as values and \AD{Tm} as computations. The
two first constraints on \AF{Var} described earlier are trivially
satisfied. Because renaming strictly respects the structure of the
term it goes through, the algebra is implemented using \AF{fmap}.
When dealing with the body a binder, we simply `reify' the
\AF{Kripke} function by evaluating it in an extended context and
feeding it dummy values corresponding to the extra variables
introduced by that context. This is reminiscent both of what we
did in Section~\ref{section:primer-program} and the definition
of reification in the setting of normalisation by evaluation
(see e.g. Coquand's work~\citeyear{coquand2002formalised}).

Substitution can be defined in a similar manner. Of the two
constraints applying to terms as values, the first one corresponds
precisely to renaming and the second one is trivial. The algebra
can once more be defined by using \AF{fmap} and reifying the bodies
of binders.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Semantics.tex]{renaming}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Semantics.tex]{substitution}
\end{minipage}
\caption{Generic Renaming and Substitution for All Scope Safe Syntaxes with Binding}
\end{figure}

The reification process mentioned in the definition of renaming
and substitution can be implemented generically for \AR{Sem}antics
families which have \AR{VarLike} values i.e. values that are thinnable
and such that we can craft dummy ones in non-empty contexts.

\begin{figure}[h]
\ExecuteMetaData[varlike.tex]{varlike}
\caption{\AR{VarLike}: \AF{Thinnable} and with dummy values}
\end{figure}

Because for any \AR{VarLike} \AB{ùì•}, we can define \AF{fresh^{r}} of
type {(\AB{Œì} \AR{‚îÄEnv}) \AB{ùì•} (\AB{Œî} \AF{++} \AB{Œì})} and \AF{fresh^{l}} of
type {(\AB{Œì} \AR{‚îÄEnv}) \AB{ùì•} (\AB{Œì} \AF{++} \AB{Œî})} by combining the use
of dummy variables and thinnings, and we can very easily prove that variables
are \AR{VarLike} we can then write \AF{reify} like so:

\begin{figure}[h]
\ExecuteMetaData[Generic/Semantics.tex]{reify}
\caption{Generic Reification thanks to \AR{VarLike} Values}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OTHER GENERIC FUNCTIONS


\section{Other Generic Programs}

One of the advantages of having a universe of programming languages
descriptions is the ability to concisely define an \emph{extension}
of an existing language by using \AD{Desc}ription transformers
grafting extra constructors √† la Swiestra~\citeyear{swierstra_2008}.
This is made extremely simple by the
disjoint sum combinator \AF{\_`+\_} which we have already seen
in Section~\ref{desccomb}.
An example of such an extension is the addition of let-bindings to
an existing language.

\subsection{Sugar and Desugaring as a Semantics}\label{section:letbinding}

Let bindings allow the user to avoid repeating herself by naming
sub-expressions and then using these names to refer to the associated
terms. Preprocessors adding these type of mechanisms to existing
languages (from C to CSS) are rather popular. We introduce \AF{Let},
a description of such let bindings which can be used to extend any
language description \AB{d} to \AF{Let} \AF{`+} \AB{d}:

\begin{figure}[h]
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{letcode}
\caption{Parallel Let Binding}
\end{figure}

In a dependently typed language a type may depend on a value which
in the presence of let bindings may be a variable standing for an
expression. The user naturally does not want it to make any difference
whether she used a variable referring to a let-bound expression or
the expression itself. Various typechecking strategies can accomodate
this expectation: in Coq~\cite{Coq:manual} let bindings are primitive
constructs of the language and have their own typing and reduction
rules whereas in Agda they are elaborated away to the core language
by inlining.

This latter approach to extending a language \AB{d} with let bindings
by inlining them before typechecking can be implemented generically as
a Semantics over (\AF{Let} \AF{`+} \AB{d}) where values in the environment and
computations both are let-free terms. The algebra of that semantics can
be defined by parts: the old constructors are simply interpreted using
the algebra defined generically for the \AF{Substitution} semantics whilst
the newer one precisely provides the extra values to be added to the
environment (we leave the definition of \AF{alg'} out because of a lack
of space). The process of removing let binders is kickstarted with a
dummy environment associating each variable to itself.

\begin{figure}[h]
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{unletcode}
\end{minipage}\hspace{2em}
\begin{minipage}{0.45\textwidth}
  \ExecuteMetaData[Generic/Examples/ElaborationLet.tex]{unlet}
\end{minipage}
\caption{Inlining Let Binding}
\end{figure}

In about 20 lines of code we have defined a generic extension of
syntaxes with binding together with a semantics which corresponds
to an elaborator translating away this new constructor. In their
own setting working on STLC, Allais, Chapman, McBride
and McKinna~\citeyear{allais2017type} have shown that it is similarly
possible to implement a Continuation Passing Style transformation as
a semantics.

We have demonstrated how easily one can define extensions and combine
them on top of a base language without having to reimplement common
traversals for each one of the intermediate representations. Moreover,
it is possible to define \emph{generic} transformations elaborating
these added features in terms of lower-level ones. This suggests that
this setup could be a good candidate to implement generic compilation
passes and could deal with a framework using a wealth of slightly
different intermediate languages √† la nanopass~\cite{Keep:2013:NFC:2544174.2500618}.

\subsection{(Unsafe) Normalisation by Evaluation}

A key type of traversal we have not studied yet is a language's
evaluator. Our universe of syntaxes with binding does not impose
any typing discipline on the user-defined languages and as such
cannot guarantee their totality. This is embodied by one of our running
examples: the untyped $\lambda$-calculus. As a consequence there
is no hope for a safe generic framework to define normalisation
functions.

The clear connection between the \AF{Kripke} functional space
characteristic of our semantics and the one that shows up in
normalisation by evaluation suggests we ought to manage to
give an unsafe generic framework for normalisation by evaluation.
By temporarily \textbf{disabling Agda's positivity checker},
we can define a generic reflexive domain \AD{Dm} in which to
interpret our syntaxes. It has three constructors corresponding
respectively to a free variable, a constructor's counterpart where
scopes have become \AF{Kripke} functional spaces on \AD{Dm} and
an error token because the evaluation of untyped programs may
(and usually does!) go wrong.

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{domain}
\caption{Generic Reflexive Domain}
\end{figure}

This datatype definition is utterly unsafe. The more conservative
user will happily restrict herself to typed settings where the
domain can be defined as a logical predicate or opt instead for
a step-indexed approach.

But this domain does make it possible to define a generic \AF{nbe}
semantics which, given a term, produces a value in the reflexive
domain. Because we have picked a universe of finitary syntaxes, we
can \emph{traverse}~\cite{mcbride_paterson_2008} the functor to define
a (potentially failing) reification function turning elements of the
reflexive domain into terms. By composing them, we obtain the
normalisation function which gives its name to normalisation by
evaluation.

The user still has to explicitly pass an interpretation of
the various constructors because there is no way for us to
know what the binders are supposed to represent: they may
stand for $\lambda$-abstractions, $\Sigma$-types, fixpoints, or
anything else she may want to define.


\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{nbe-setup}
\caption{Generic Normalisation by Evaluation Framework}
\end{figure}

Using this setup, we can write a normaliser for the untyped
$\lambda$-calculus: we use \AF{case} to distinguish between
the semantical counterpart of the application constructor on
one hand and the $\lambda$-abstraction one on the other.
The latter is trivial: functions are already
values! The semantical counterpart of application proceeds by
case analysis on the function: if it corresponds to a
$\lambda$-abstraction, we can fire the redex by using the Kripke
functional space; otherwise we grow the spine of stuck
applications.

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/NbyE.tex]{nbelc}
\caption{Normalisation by Evaluation for the Untyped $\lambda$-Calculus}
\end{figure}

We haven't used the \AIC{‚ä•} constructor so \emph{if} the evaluation terminates
(by disabling totality checking we have lost all guarantees with respect to
termination) we know we will get a term in normal form.

\subsection{An Algebraic Approach to Typechecking}

Following Bob Atkey~\citeyear{atkey2015algebraic}, we can consider type checking
and type inference as a possible semantics for a bi-directional~\cite{pierce2000local}
language. We represent the raw syntax of a simply-typed bi-directional calculus
as a bi-sorted language using a notion of \AD{Phase} to distinguish between terms
for which we will be able to \AIC{Infer} the type and the ones for which we will
have to \AIC{Check} a type candidate.

Following traditional presentations, eliminators give rise to \AIC{Infer}rable
terms under the condition that the term they are eliminating is also \AIC{Infer}rable
and the other arguments are \AIC{Check}able whilst constructors are always \AIC{Check}able.
Two extra constructors allow changes of direction: \AIC{Cut} annotates a \AIC{Check}able
term with its type thus making it \AIC{Infer}rable whilst \AIC{Emb} embeds \AIC{Infer}rables
into \AIC{Check}ables.

\begin{figure}[h]
\begin{minipage}{0.35\textwidth}
  \ExecuteMetaData[Generic/Examples/TypeChecking.tex]{constructors}
\end{minipage}
\begin{minipage}{0.55\textwidth}
  \ExecuteMetaData[Generic/Examples/TypeChecking.tex]{bidirectional}
\end{minipage}
\caption{A Bidirectional Simply-Typed Language}
\end{figure}

The values stored in the environment will be \AD{Type} information for bound
variables no matter what their \AD{Phase} is. In constrast, the generated
computations will, depending on the phase, either take a type candidate and
\AIC{Check} it is valid or \AIC{Infer} a type for their argument. These
computations are always potentially failing so we use the \AD{Maybe} monad.

\begin{figure}[h]
\begin{minipage}{0.40\textwidth}
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{varmode}
\end{minipage}
\begin{minipage}{0.50\textwidth}
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{typemode}
\end{minipage}
\caption{Var- and Type- Modes indexed by the Phase}
\end{figure}

We can now define typechecking as a \AR{Sem}antics. The algebra describes the
algorithm:
\begin{itemize}
  \item when facing an application: infer the type of the function, make sure
    it is an arrow type, check the argument at the domain's type and return the
    codomain
  \item for a $\lambda$-abstraction: check the input type is an arrow type and
    check the body at the codomain type in the extended environmnent where the
    newly-bound variable as the domain's type
  \item a cut always comes with a type candidate against which to check the term
    and to be returned in case of success
  \item finally, the change of direction from Inferrable to Checkable is successful
    when the inferred term is equal to the expected one.
\end{itemize}

\begin{figure}[h]
\ExecuteMetaData[Generic/Examples/TypeChecking.tex]{typecheck}
\caption{Type- Inference / Checking as a Semantics}
\end{figure}

In about a dozen lines of code we have defined a bidirectional typechecker for
this simple language by leveraging the \AF{Sem}antics framework. The code attached
to this paper also contains a variant with more informative types: instead of simply
generating a type or checking that a candidate will do, we can use our \AD{Desc}riptions
to describe a language of evidence and generate not only an expression's type but
also a well scoped and well typed term of that type.

\subsection{Binding as Self-Reference: Representing Cyclic Structures}

Ghani, Hamana, Uustalu and Vene~\citeyear{ghani2006representing} have
demonstrated how Altenkirch and Reus' type-level de Bruijn
indices~\citeyear{altenkirch1999monadic} can be used to represent
potentially cyclic structures by a finite object. In their
representation each bound variable is a pointer to the node
that introduced it. Because we are, at the top-level, only
interested in structures with no ``dangling pointers'', we introduce
the notation \AF{TM} \AB{d} to mean closed terms (i.e. terms of type
\AD{Tm} \AB{d} \AF{‚àû} \AIC{[]}).

A basic example of such a structure is a potentially cyclic list which
offers a choice of two constructors: \AIC{[]} which ends the list and
\AIC{\_:\!:\_} which combines a head and a tail but also acts as a binder
for a self-reference. We can see this approach in action in the examples
\AF{[0, 1]} and \AF{01‚Ü∫} which describe respectively a finite list containing
0 followed by 1 and a cyclic list starting with 0, then 1, and then
repeating the whole list again by referring to the first cons cell
represented here by the de Bruijn variable 1 (i.e. \AIC{s} \AIC{z}).

\begin{figure}[h]
\begin{minipage}{0.55\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{clistD}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{clistpat}
\end{minipage}\hspace{2em}
\begin{minipage}{0.35\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{zeroones}
\end{minipage}
\caption{Potentially Cyclic Lists: Description, Pattern Synonyms and Examples}
\end{figure}

These finite representations are interesting in their own right
and we can use the generic semantics framework defined earlier
to manipulate them. A basic building block is the \AF{unroll}
function which takes a closed tree, exposes its top node and
unrolls any cycle which has it as its starting point. We can
decompose it using the \AF{plug} function which, given a closed
and an open term, closes the latter by plugging the former at
each free \AIC{`var} leaf. Because \AF{plug}'s fundamental nature
is that of substituting a term for each leaf, it can naturally
be implemented using \AF{Substitution}.

\begin{figure}[h]
\begin{minipage}{0.52\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{plug}
\end{minipage}\hspace{2em}
\begin{minipage}{0.43\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{unroll}
\end{minipage}
\caption{Plug and Unroll: Exposing a Cyclic Tree's Top Layer}
\end{figure}

However, one thing still out of our reach with the current tools we have
is the underlying co-finite trees these finite objects are meant
to represent. We can start by defining the coinductive type
corresponding to them as the greatest fixpoint of a notion of
layer. One layer of a co-finite tree is precisely given by the
meaning of its description where we completely ignore the binding
structure. We show with \AF{01‚ãØ} which infinite list ought to
correspond to the cyclic example \AF{01‚Ü∫} given above.

\begin{figure}[h]
\begin{minipage}{0.55\textwidth}
  \ExecuteMetaData[Generic/Cofinite.tex]{cotm}
\end{minipage}\hspace{2em}
\begin{minipage}{0.35\textwidth}
  \ExecuteMetaData[Generic/Examples/Colist.tex]{zeroones2}
\end{minipage}
\caption{Co-finite Trees: Definition and Example}
\end{figure}

We can then make the connection between potentially cyclic
structures and the co-finite trees formal by giving an \AF{unfold}
function which, given a closed term, produces its unfolding.
The definition proceeds by unrolling the term's top layer and
co-recursively unfolding all the subterms.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Cofinite.tex]{unfold}
\caption{Generic Unfold of Potentially Cyclic Structures}
\end{figure}

We can see that this universe of descriptions allows us to
implement generic functions once and for all. Even if the
powerful notion of semantics described in Section~\ref{section:semantics}
cannot encompass all the traversals we may be interested in,
it provides us with reusable building blocks: the definition
of \AF{unfold} was made very simple by reusing the generic
program \AF{fmap} and the \AF{Substitution} semantics whilst
the definition of \AR{‚àûTm} was made easy by reusing \AF{‚ü¶\_‚üß}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GENERIC PROOFS


\section{Building Generic Proofs about Generic Programs}\label{section:simulation}

Allais, Chapman, McBride, and McKinna~\citeyear{allais2017type} have
already shown that, for their specific language, introducing an abstract
notion of Semantics not only reveals the shared structure of common
traversals, it also allows them to give abstract proof frameworks for
simulation or fusion lemmas. Their idea naturally extends to our generic
presentation of semantics for all syntaxes.

The most important concept in this section is (\AF{Zip} \AB{d}), a relation
transformer which characterises structurally equal layers such that their
substructures are themselves related by the relation it is passed as an
argument. It inherits a lot of its relational arguments' properties: whenever
\AB{R} is reflexive (respectively symmetric or transitive) so is {\AF{Zip} \AB{d} \AB{R}}.

It is defined by induction on the description and case analysis on the two
layers which are meant to be equal:
\begin{itemize}
  \item In the stop token case \AIC{`‚àé} \AB{i}, the two layers are considered to
    be trivially equal (i.e. the constraint generated is the unit type)
  \item When facing a recursive position {\AIC{`X} \AB{\Delta} \AB{j} \AB{d}}, we
    demand that the two substructures are related by {\AB{R} \AB{\Delta} \AB{j}}
    and that the rest of the layers are related by \AF{Zip} \AB{d} \AB{R}
  \item Last but not least, two nodes of type {\AIC{`\sigma} \AB{A} \AB{d}} will
    be related if they both carry the same payload \AB{a} of type \AB{A} and if
    the rest of the layers are related by {\AF{Zip} (\AB{d} \AB{a}) \AB{R}}
\end{itemize}

\begin{figure}[h]
 \ExecuteMetaData[Generic/Zip.tex]{ziptype}
\caption{Zip: Characterising Structurally Equal Values with Related Substructures}
\end{figure}

If we were to take a fixpoint of \AF{Zip}, we could obtain a structural
notion of equality for terms which would be equivalent to propositional
equality. In the section we are however interested in more advanced
use-cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SIMULATION


\subsection{Simulation Lemma}

A \AF{Zip} constraint appears naturally when we want to say that a
semantics can simulate another one. Given a relation \AB{ùì°^ùì•} connecting values
in \AB{ùì•_1} and \AB{ùì•_2}, and a relation \AB{ùì°^ùìí} connecting computations in
\AB{ùìí_1} and \AB{ùìí_2}, we can define \AF{Kripke^R} relating values
\AF{Kripke} \AB{ùì•_1} \AB{ùìí_1} and \AF{Kripke} \AB{ùì•_2} \AB{ùìí_2}
by stating that they send related inputs to related outputs. We use
the relation transformer \AF{‚àÄ[\_]} which lifts a relation on values
to one on environments in a pointwise manner.

\begin{figure}[h]
 \ExecuteMetaData[varlike.tex]{kripkeR}
\caption{Relational Kripke Function Spaces: From Related Inputs to Related Outputs}
\end{figure}

We can then combine \AF{Zip} and \AF{Kripke^R} to express the idea
that two semantic objects of respective types
\AF{‚ü¶} \AB{d} \AF{‚üß} (\AF{Kripke} \AB{ùì•_1} \AB{ùìí_1}) and
\AF{‚ü¶} \AB{d} \AF{‚üß} (\AF{Kripke} \AB{ùì•_2} \AB{ùìí_2}) are
synchronised. The simulation constraint on two \AF{Sem}antics' algebras
then becomes: given synchronized objects, the algebras should yield
related computations. Together with self-explanatory constraints on
\ARF{var} and \ARF{th^ùì•}, this constitutes the whole \AF{Sim}ulation
constraint:

\begin{figure}[h]
 \ExecuteMetaData[Generic/Simulation.tex]{recsim}
\caption{A Generic Notion of Simulation}
\end{figure}

The fundamental lemma of simulations is a generic theorem showing that for
each pair of \AR{Sem}antics respecting the \AR{Sim}ulation constraint, we
get related computations given environments of related input values. This
theorem is once more mutually proven with a statement about \AF{Scope}s,
and \AD{Size}s play a crucial role in ensuring that the function is indeed total.

\begin{figure}[h]
\ExecuteMetaData[Generic/Simulation.tex]{simbody}
\caption{Fundamental Lemma of \AF{Sim}ulations}
\end{figure}

Instantiating this generic simulation lemma, we can for instance get
that Renaming and Substitution are extensional (given extensionally
equal environments they produce syntactically equal terms), or that
Renaming is a special case of Substitution. Of course these results
are not new but having them generically over all syntaxes with binding
is convenient; which we have experienced first hand when tackling the
POPLMark reloaded challenge where \AF{rensub} was actually needed.

\begin{figure}[h]
\ExecuteMetaData[Generic/Simulation.tex]{rensub}
\caption{Renaming as a Substitution via Simulation}
\end{figure}

When studying specific languages, new opportunities to deploy the
fundamental lemma of simulations arise. Our solution to the POPLMark
reloaded challenge for instance describes the fact that {\AF{sub} \AB{\rho} \AB{t}}
reduces to {\AF{sub} \AB{\rho'} \AB{t}} whenever for all \AB{v},
\AB{\rho}(\AB{v}) reduces to \AB{\rho'}(\AB{v}) as a \AR{Sim}ulation.
The main theorem (strong normalisation of STLC via a logical relation)
is itself an instance of (the unary version of) the simulation lemma.

This Simulation proof framework is the simplest examples of the kind of abstract
proof frameworks Allais, Chapman, McBride and McKinna introduce for their
specific language. They also explain how a similar framework can be defined
for fusion lemmas and deploy it for the renaming-substitution interactions
but also their respective interactions with normalisation by evaluation.
Now that we are familiarised with the techniques at hand, we can tackle
this more complex example.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FUSION

\subsection{Fusion Lemma}

Results which can be reformulated as the ability to fuse two traversals
obtained as \AF{Semantics} into one abound. When claiming that \AF{Tm} is
a Functor, we have to prove that two successive renamings can be fused into
a single renaming where the \AF{Thinning}s have been composed. Similarly,
demonstrating that \AF{Tm} is a relative Monad~\cite{JFR4389} implies proving
that two consecutive substitutions can be merged into a single one whose
environment is the first one, where the second one has been applied in a
pointwise manner. Last but not least, the \emph{Substitution Lemma} central
to most model constructions (see for instance~\cite{mitchell1991kripke}) states
that a syntactic substitution followed by the evaluation of the resulting term
into the model is equivalent to the evaluation of the original term with an
environment corresponding to the evaluated substitution.

Because all of these statements have precisely the same structure, we can
once more devise a framework which will, provided that its constraints are
satisfied, prove a generic fusion lemma.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Fusion.tex]{fusion}
\caption{A Generic Notion of Fusion}
\end{figure}

A direct application of these results is our (to be published) entry to the
POPLMark Challenge Reloaded~\citeyear{poplmarkreloaded}. By using a \AD{Desc}-based
representation of intrisically well-typed and well-scoped terms we directly inherit
not only renaming and substitution but also all four fusion lemmas as corollaries
of our generic results. This allows us to shave off a lot of the usual boilerplate
and go straight to the point.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Fusion.tex]{fusbody}
\caption{Fundamental Lemma of \AR{Fus}ion}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BISIMILARITY

\subsection{Definition of Bisimilarity for co-finite objects}

Although we were able to use propositional equality when studying
syntactic traversals working on terms, it is not the appropriate
notion of equality for co-finite trees. We can use \AF{Zip} to
define generically a coinductive notion of bisimilarity for all
co-finite trees obtained as unfolding of descriptions. And we
can derive from \AF{Zip}'s properties the fact that this gives
rise to an equivalence relation.

\begin{figure}[h]
 \ExecuteMetaData[Generic/Bisimilar.tex]{bisim}
 \ExecuteMetaData[Generic/Bisimilar.tex]{eqrel}
\caption{Bisimilarity, an Equivalence Relation for Co-finite Trees}
\end{figure}



We have also seen how this setup
could be applied to a different domain: the representation of potentially
cyclic structures by finite artefacts.


The potentially cyclic structures we have studied have been improved
upon by Hamana~\citeyear{Hamana2009} who gave a presentation which
preserves sharing: pointers can not only refer to nodes above them
but also across from them in the cyclic tree. This yields a much more
involved binding structure which would be interesting in its own right.


\section{Fully Worked-out Example}

\todo{system F, etc.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RELATED WORK

\section{Related Work}

\subsection{Variable Binding} The representation of variable binding
in formal systems has been a hot topic for decades. Part of the purpose
of the first POPLMark challenge~\citeyear{poplmark} was to explore and
compare various methods. Our generic universe of syntax is based on
scope-and-typed de Bruijn indices~\cite{de1972lambda} but it is not
a vital necessity. It is for instance possible to give an interpretation
of \AD{Desc}ripton corresponding to Chlipala's Parametric Higher-Order
Abstract Syntax~\citeyear{chlipala2008parametric} and we would be interested
to see what the appropriate notion of Semantics is for this representation.

The practice of enforcing strong invariants in the deeply-embedded syntax
of the programming language to be studied has recently been extended to
imperative languages~\citeyear{BachPoulsen}.

\subsection{Meta-Theory Automation via Tactics} The tediousness of repeatedly
proving similar statements has unsurprisingly led to various attempts at
automating the pain away via either code generation or the definition of
tactics. These solutions can be seen as untrusted oracle driving the
interactive theorem prover.

Polonowski's DBGen~\citeyear{polonowski:db} takes as input a raw syntax with
comments annotating binding sites. It generates a module defining lifting,
substitution as well as a raw syntax using names and a validation function
transforming named terms into de Bruijn ones; we refrain from calling it a
scopechecker as terms are not statically proven to be well-scoped.

Kaiser, Sch√§fer, and Stark~\citeyear{Kaiser-wsdebr} build on our previous paper
to draft possible theoretical foundations for Autosubst, a so far untrusted
set of tactics. The whole paper is based on a specific syntax: well-scoped
call by value System F. In contrast, our effort has been here to carve out
a precise universe of syntaxes with binding and give a systematic account
of their semantics and proofs.

Keuchel, Weirich, and Schrijvers' Needle~\citeyear{needleandknot} is a code
generator written in Haskell producing syntax-specific Coq modules
implementing common traversals and lemmas about them.

\subsection{Universes of Syntaxes with Binding} Keeping in mind Altenkirch
and McBride's observation that generic programming is everyday programming
in dependently-typed languages~\citeyear{genericprogramming-dtp}, we can naturally
expect generic, provably sound, treatments of these notions in tools such as
Agda or Coq.

Keuchel, Weirich, and Schrijvers' Knot presented in the same paper as
Needle~\citeyear{needleandknot} implements the traversals and lemmas generated
in specialised forms by Needle once and for all as a set of generic programs.
They see the existence of Needle as a pragmatic choice: working directly with
the free monadic terms over finitary containers would be too cumbersome. In
our experience solving the POPLMark reloaded challenge, Agda's pattern
synonym~\cite{Pickering:patsyn} make working with an encoded definition almost
seamless.

The GMeta generic framework~\citeyear{gmeta} does provide a universe of syntaxes
and offers various binding conventions (locally nameless~\cite{Chargu√©raud2012}
or de Bruijn indices). It also generically implements common traversals (computing
the sets of free / bound variables, measuring the size of a term, shifting
de Bruijn indices or substituting terms for parameters) as well as common
predicates (e.g. being a closed term) and provide generic lemmas proving that
all of these are well-behaved. It does not however offer a generic framework
for defining new well-scoped-and-typed semantics and proving that they are
well-behaved.

√ârdi~\citeyear{gergodraft} defines a universe inspired by ours and gives three
different interpretations corresponding to raw, scoped and typed syntax
which are related via erasure. He provides scope-and-type preserving renaming
and substitution as well as various generic proofs that they are well-behaved.

Copello~\citeyear{copello2017} sets out to work with \emph{named} binders and
defines nominal techniques (such as name swapping) and ultimately $\alpha$-equivalence
over a generic universe of regular trees with binders inspired by Morris'~\citeyear{morris-regulartt}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONCLUSION

\section{Conclusion}

We have started from an example of a scope safe language (the
untyped $\lambda$-calculus), have studied various common traversals
and noticed their similarity. After introducing a notion of semantics
and refactoring these traversals as various instances of the same
fundamental lemma, we have observed the tight connection between the
abstract definition of semantics and the shape of the language. By
extending a universe of datatype descriptions to support a notion of
binding, we have managed to give a generic presentation of syntaxes
with binding as well as a large class of scope safe programs acting
on them: from Renaming and Substitution, to Normalisation by Evaluation,
and the Desugaring of new constructors added by a language transformer.
Last but not least, we have seen how to construct generic proofs about
these generic programs. The diverse influences leading to this body of
work suggest many opportunities for future research:

The question of a universe of syntaxes with binding which are not only
well scoped but also intrinsically well typed by construction is an
exciting challenge. The existing variation on the universe of datatypes
giving a universe of inductive families~\cite{dybjer1994inductive}
is a natural candidate.

Our example of the elaboration of an enriched language to a core one,
and Allais, Chapman, McBride and McKinna's implementation of a
Continuation Passing Style conversion function begs the question of how
many such common compilation passes can be implemented generically.
An extension of McBride's theory of ornaments~\citeyear{mcbride2010ornamental}
could provide an appropriate framework to highlight the connection
between various languages, some being seen as the extension of others.

\newpage
